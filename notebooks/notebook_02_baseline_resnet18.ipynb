{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b445eded",
   "metadata": {},
   "source": [
    "Imports and Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c798eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b376f1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"using device\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f7d73b",
   "metadata": {},
   "source": [
    "Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0be63c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../data/raw\")\n",
    "TRAIN_DIR = DATA_DIR / \"training\"\n",
    "VAL_DIR = DATA_DIR / \"validation\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe2c979",
   "metadata": {},
   "source": [
    "Image Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c725b863",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "\n",
    "])\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0e65d2",
   "metadata": {},
   "source": [
    "Dataset and DataLoader\n",
    "- using ImageFolder: it's a PyTorch built in function which assign images labels acc to their folder names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a60d8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: {'Bread': 0, 'Dairy product': 1, 'Dessert': 2, 'Egg': 3, 'Fried food': 4, 'Meat': 5, 'Noodles-Pasta': 6, 'Rice': 7, 'Seafood': 8, 'Soup': 9, 'Vegetable-Fruit': 10}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.ImageFolder(TRAIN_DIR, transform=train_transforms)\n",
    "val_dataset = datasets.ImageFolder(VAL_DIR, transform=val_transforms)\n",
    "\n",
    "class_to_idx = train_dataset.class_to_idx\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "print(\"Classes:\", class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92c78a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 9866\n",
       "    Root location: ../data/raw/training\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
       "               RandomHorizontalFlip(p=0.5)\n",
       "               RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)\n",
       "               ToTensor()\n",
       "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "           )"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd640ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"class_mapping.json\", \"w\") as f:\n",
    "    json.dump(idx_to_class, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baec1766",
   "metadata": {},
   "source": [
    "DataLoader: how data is going to fed up in the model\n",
    "- num_workers = 2, tells how many cpu processes load data in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c83a1823",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d099ef",
   "metadata": {},
   "source": [
    "Load ResNet18\n",
    "- we freeze some layers because originally resnet buil on 1000 classes so we will our case imp layers only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5791f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/foodsense/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Applications/anaconda3/envs/foodsense/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "#Freeze backbone\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#replace classifier\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 11)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5e7174",
   "metadata": {},
   "source": [
    "Loss And Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7da25947",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.fc.parameters(),\n",
    "    lr=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41fb2bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c0e95c",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32c1cc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0, 0, 0\n",
    "    for images, labels in tqdm(loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return running_loss/len(loader), correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be285ace",
   "metadata": {},
   "source": [
    "Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94b0371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        return running_loss/len(loader), correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d856c546",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/309 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/foodsense/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "100%|██████████| 309/309 [00:36<00:00,  8.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Epoch 1/8\n",
      "    Train Loss: 2.0690 | Train Acc: 0.2878\n",
      "    Val Loss:   1.7656 | Val Acc:   0.4359\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 309/309 [00:36<00:00,  8.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Epoch 2/8\n",
      "    Train Loss: 1.5944 | Train Acc: 0.5257\n",
      "    Val Loss:   1.4236 | Val Acc:   0.5956\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 309/309 [00:38<00:00,  7.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Epoch 3/8\n",
      "    Train Loss: 1.3441 | Train Acc: 0.6243\n",
      "    Val Loss:   1.2346 | Val Acc:   0.6501\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 309/309 [00:39<00:00,  7.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Epoch 4/8\n",
      "    Train Loss: 1.1876 | Train Acc: 0.6624\n",
      "    Val Loss:   1.1233 | Val Acc:   0.6738\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 309/309 [00:37<00:00,  8.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Epoch 5/8\n",
      "    Train Loss: 1.0894 | Train Acc: 0.6914\n",
      "    Val Loss:   1.0335 | Val Acc:   0.6921\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 309/309 [00:36<00:00,  8.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Epoch 6/8\n",
      "    Train Loss: 1.0107 | Train Acc: 0.7076\n",
      "    Val Loss:   0.9658 | Val Acc:   0.7111\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 309/309 [00:39<00:00,  7.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Epoch 7/8\n",
      "    Train Loss: 0.9567 | Train Acc: 0.7149\n",
      "    Val Loss:   0.9111 | Val Acc:   0.7219\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 309/309 [00:38<00:00,  7.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Epoch 8/8\n",
      "    Train Loss: 0.9127 | Train Acc: 0.7315\n",
      "    Val Loss:   0.8776 | Val Acc:   0.7315\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 8\n",
    "best_val_acc = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader)\n",
    "    val_loss, val_acc = validate(model, val_loader)\n",
    "\n",
    "    print(f\"\"\"\n",
    "    Epoch {epoch+1}/{EPOCHS}\n",
    "    Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\n",
    "    Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\n",
    "    \"\"\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foodsense",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
